"""Example: Using SimpleRunner with dynamically generated annotation files.

This demonstrates a realistic ML pipeline where annotation files are generated
on-the-fly from upstream data processing or experiment tracking systems.

Realistic Use Cases:
====================

1. **MLflow Experiment Tracking**
   - Annotation files downloaded from MLflow artifact store
   - Each experiment run gets unique versioned annotation files
   - No manual file management needed

2. **Apache Airflow Data Pipeline**
   - Data processing jobs generate annotation files daily
   - Training pipeline pulls latest generated files
   - Easy to integrate with production ETL workflows

3. **Cross-Validation Workflows**
   - Generate K different train/val splits programmatically
   - Iterate over folds for robust model evaluation
   - No need to manually create separate preset files

4. **A/B Testing and Experimentation**
   - Generate different data distributions on-the-fly
   - Compare model performance across variations
   - Version-controlled experiment configurations

5. **CI/CD Integration Testing**
   - Generate small synthetic datasets for fast test runs
   - Validate pipeline works before committing
   - Avoid slow tests on full production datasets

6. **DVC (Data Version Control)**
   - Pull versioned datasets based on git tags
   - Annotation files automatically updated with data versions
   - Full reproducibility of training runs
"""

import json
import tempfile
from pathlib import Path

from visdet import SimpleRunner


def generate_annotation_file(output_path: Path, num_samples: int = 10, num_classes: int = 1) -> None:
    """Simulate upstream pipeline generating COCO annotation file.

    In production, this might be:
    - MLflow artifact download
    - Data processing job output
    - Cross-validation fold generator
    - A/B test configuration
    - DVC-tracked dataset

    Args:
        output_path: Where to write the annotation file
        num_samples: Number of images to generate
        num_classes: Number of classes in the dataset
    """
    annotation = {
        "images": [
            {"id": i, "file_name": f"image_{i:04d}.jpg", "width": 640, "height": 480} for i in range(num_samples)
        ],
        "annotations": [
            {
                "id": i,
                "image_id": i,
                "category_id": (i % num_classes) + 1,
                "bbox": [100, 100, 200, 200],
                "area": 40000,
                "iscrowd": 0,
                "segmentation": [[100, 100, 300, 100, 300, 300, 100, 300]],
            }
            for i in range(num_samples)
        ],
        "categories": [{"id": i + 1, "name": f"class_{i}", "supercategory": "thing"} for i in range(num_classes)],
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(annotation, indent=2))
    print(f"✓ Generated annotation file: {output_path}")


def example_mlflow_workflow():
    """Example: Training with MLflow experiment artifacts."""
    print("\n" + "=" * 70)
    print("EXAMPLE 1: MLflow Experiment Artifacts")
    print("=" * 70)
    print(
        """
In production, you would:
1. Run experiment in MLflow (logs model and annotations)
2. Query MLflow API to get latest run artifacts
3. Download annotation files from artifact store
4. Train with downloaded files

Code:
    import mlflow
    client = mlflow.tracking.MlflowClient()

    # Get latest experiment run
    runs = client.search_runs(experiment_ids=[exp_id])
    latest_run = runs[0]

    # Download artifacts
    train_ann = client.download_artifacts(latest_run.info.run_id, 'train.json')
    val_ann = client.download_artifacts(latest_run.info.run_id, 'val.json')

    # Train with dynamic files
    runner = SimpleRunner(
        model='mask_rcnn_swin_s',
        dataset='cmr_instance_segmentation',
        train_ann_file=train_ann,
        val_ann_file=val_ann,
        epochs=12
    )
    runner.train()
    """
    )


def example_cross_validation():
    """Example: Cross-validation with programmatically generated folds."""
    print("\n" + "=" * 70)
    print("EXAMPLE 2: Cross-Validation with Generated Folds")
    print("=" * 70)

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        folds_dir = temp_path / "folds"
        folds_dir.mkdir()

        print("\nGenerating 3 cross-validation folds...")

        # Simulate 3-fold cross-validation
        for fold_idx in range(3):
            train_ann = folds_dir / f"fold_{fold_idx}_train.json"
            val_ann = folds_dir / f"fold_{fold_idx}_val.json"

            # In reality, these would be generated by your fold splitting logic
            generate_annotation_file(train_ann, num_samples=80)
            generate_annotation_file(val_ann, num_samples=20)

            print(f"\n▶ Training on Fold {fold_idx + 1}/3")
            print(f"  Train: {train_ann}")
            print(f"  Val:   {val_ann}")

            # Train with fold-specific files
            _runner = SimpleRunner(  # noqa: F841
                model="mask_rcnn_swin_s",
                dataset="cmr_instance_segmentation",
                train_ann_file=str(train_ann),
                val_ann_file=str(val_ann),
                epochs=1,  # Short run for demo
                work_dir=str(temp_path / f"fold_{fold_idx}" / "work_dirs"),
            )

            # Uncomment to actually train:
            # _runner.train()
            print("  ✓ Ready to train (training skipped in example)")


def example_ab_testing():
    """Example: A/B testing with different data distributions."""
    print("\n" + "=" * 70)
    print("EXAMPLE 3: A/B Testing Different Data Distributions")
    print("=" * 70)

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        experiments_dir = temp_path / "experiments"
        experiments_dir.mkdir()

        experiments = {
            "baseline": {"num_samples": 100, "num_classes": 70},
            "augmented": {"num_samples": 200, "num_classes": 70},
            "balanced": {"num_samples": 150, "num_classes": 70},
        }

        print("\nGenerating annotation files for A/B test variants...\n")

        for exp_name, config in experiments.items():
            exp_dir = experiments_dir / exp_name
            exp_dir.mkdir()

            train_ann = exp_dir / "train.json"
            val_ann = exp_dir / "val.json"

            # Generate variant-specific files
            generate_annotation_file(
                train_ann,
                num_samples=config["num_samples"],
                num_classes=config["num_classes"],
            )
            generate_annotation_file(
                val_ann,
                num_samples=config["num_samples"] // 5,
                num_classes=config["num_classes"],
            )

            print(f"\nVariant: {exp_name.upper()}")
            print(f"  Config: {config}")
            print(f"  Train annotations: {train_ann}")
            print(f"  Val annotations:   {val_ann}")

            # Train variant
            _runner = SimpleRunner(  # noqa: F841
                model="mask_rcnn_swin_s",
                dataset="cmr_instance_segmentation",
                train_ann_file=str(train_ann),
                val_ann_file=str(val_ann),
                epochs=1,  # Short run for demo
                work_dir=str(exp_dir / "work_dirs"),
            )

            # Uncomment to train all variants:
            # _runner.train()
            print("  ✓ Ready to train variant")


def example_ci_cd_integration_test():
    """Example: Fast CI/CD integration testing with synthetic data."""
    print("\n" + "=" * 70)
    print("EXAMPLE 4: CI/CD Integration Testing")
    print("=" * 70)

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        test_dir = temp_path / "integration_test"
        test_dir.mkdir()

        print("\nGenerating minimal synthetic dataset for CI testing...")

        # Create small synthetic dataset for fast testing
        train_ann = test_dir / "train.json"
        val_ann = test_dir / "val.json"

        generate_annotation_file(train_ann, num_samples=5)  # Minimal dataset
        generate_annotation_file(val_ann, num_samples=2)

        print("\n▶ Running CI/CD integration test")
        print(f"  Train: {train_ann} (5 images)")
        print(f"  Val:   {val_ann} (2 images)")

        _runner = SimpleRunner(  # noqa: F841
            model="mask_rcnn_swin_s",
            dataset="cmr_instance_segmentation",
            train_ann_file=str(train_ann),
            val_ann_file=str(val_ann),
            epochs=1,
            val_interval=1,
            work_dir=str(test_dir / "work_dirs"),
        )

        # Uncomment to run actual training:
        # _runner.train()
        print("  ✓ Pipeline syntax validated (training skipped in example)")


def main():
    """Run all examples."""
    print("\n" + "=" * 70)
    print("Dynamic Annotation Files in SimpleRunner")
    print("=" * 70)
    print(
        """
The SimpleRunner API now supports train_ann_file and val_ann_file parameters
for specifying annotation files dynamically. This enables seamless integration
with ML pipelines where annotation files are generated on-the-fly from
upstream data sources.

This is essential for:
  • MLflow experiment tracking
  • Apache Airflow data pipelines
  • Cross-validation workflows
  • A/B testing and experimentation
  • CI/CD integration testing
  • DVC versioned datasets
    """
    )

    # Run example code demonstrations
    example_mlflow_workflow()
    example_cross_validation()
    example_ab_testing()
    example_ci_cd_integration_test()

    print("\n" + "=" * 70)
    print("✓ All examples completed successfully!")
    print("=" * 70)


if __name__ == "__main__":
    main()
